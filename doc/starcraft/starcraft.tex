% This is based on the LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
% See http://www.springer.com/computer/lncs/lncs+authors?SGWID=0-40209-0-0-0
% for the full guidelines.

\documentclass{llncs}

\usepackage[hyperfootnotes=true]{hyperref}
\usepackage{tabularx}
\usepackage{tikz}
\usetikzlibrary{graphs}
\usepackage{siunitx}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{clrscode3e}
\usepackage{booktabs}
\usepackage{url}

\begin{document}

\title{StarCraft AI Research Summary}
\titlerunning{}  % abbreviated title(for running head)

\author{Qinyu Chen}
\authorrunning{} % abbreviated author list(for running head)

%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{}

\institute{
\email{henryqychen@gmail.com},\\
\texttt{BeiJing, China}
}

\maketitle              % typeset the title of the contribution

%\begin{abstract}
%\keywords{}
%\end{abstract}

\section{Why StarCraft?}
\emph{StarCraft} has proven to be a challenging environment for artificial intelligence research. While hand-crafted strategies are still the state-of-art, the deep networks approach is able to
express a wide range of different strategies and thus improving
the network’s performance with deep reinforcement
learning is an immediately promising avenue for future research.\cite{DBLP:journals/corr/JustesenR17}
\subsection{Advantages}
\begin{enumerate}
\item Large-scale game replays data in the community
\item Classic RTS game with perfect designed balance, competitiveness and purity
\end{enumerate}

\subsection{Challenges}
\begin{enumerate}
\item Enormous Search Space(10 times complexer than \emph{Go} of game states and actions)
\item Partially observed environment
\item Memory preserving agent
\item Inference both in time and space
\item Multi-agent corporation
\item High time efficiency
\end{enumerate}

\section{Online Resources}
\subsection{Replays}
\emph{GosuGamers}
\footnote{\url{http://www.gosugamers.net/}},
\emph{ICCup}
\footnote{\url{https://iccup.com/}},
\emph{TeamLiquid}
\footnote{\url{http://www.teamliquid.net/}}
\subsection{Competitions}
\emph{AIIDE StarCraft AI Competition}
\footnote{\url{https://sites.google.com/view/aiide2017/}},
\emph{CIG StarCraft RTS AI Competition}
\footnote{\url{https://ls11-www.cs.tu-dortmund.de/rts-competition/start}},
\emph{Student StarCraft AI Competition}
\footnote{\url{https://sscaitournament.com/}}

\section{Game AI}
\subsection{Thread in game AI}
Usually there is only one thread to handle AI logic, otherwise there could be multi-threaded for computation and one thread for rendering.
\subsection{Difficulty level of game AI design}
The most difficult AI design is for sports game, which is very similar to robotics
\begin{eqnarray}
MMO<FPS<RTS<Sports
\end{eqnarray}

\subsection{Traditional game AI methods}
\subsubsection{State Machine}
\subsubsection{Hierarchical State Machine}
\subsubsection{Behavior Tree}

\section{RTS Game AI}
Real-time strategy(RTS) games have historaically been a domain of interest of the planning and decision making research communities.
\subsection{Macromanagement AI}
Or called General's AI which determine the time series of constructing buildings, conducting research, and producing units, among other things involving the intake and expending of resources. This is actually a form of micromanagement done to a relatively large number of units
\subsubsection{Rule-based methods}
Implemented by if-else.
\begin{enumerate}
\item TODO
\end{enumerate}

\subsubsection{Advanced methods}
\begin{enumerate}
\item planner
\item scoring system, power map
\item machine learning methods
\end{enumerate}

\subsection{Agent AI}
Or called Soldier's AI, human player's micro-operation.

\subsubsection{State Machine}
Behaviors will be triggered in certain condition(environment, resource, time)

\subsubsection{Behavior Tree}
Theoretically, the behavior tree is equivalent to state machine but could reduce the complexity of control logic.\\
Practically, behavior tree need only 1/3 lines of more readable code  comparing with the same logic implemented by FSM.

\subsubsection{Hierarchical State Machine}
This is a traditional and efficient method to reduce the complexity of game AI design.

\section{Modeling RTS Game}
\subsection{Reinforcement Learning Framework}
Reinforcement Learning (RL) is an area of machine learning inspired by behaviourist psychology, concerned with how agents should take actions in an environment so as to maximize some notion of cumulative reward.\\
There are four main reseasons make RL quite different from other machine learning paradigms:\cite{David_Silver:intro_RL}
\begin{enumerate}
\item There is no supervisor, only a reward signal
\item Feedback is delayed, not instantaneous
\item Time really matters (sequential, non i.i.d data)
\item Agent’s actions affect the subsequent data it receives
\end{enumerate}
RL focus on online performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The $E\&E$ trade-off in RL has been most thoroughly studied through the multi-armed bandit (MAB) problem and in finite MDPs.

\begin{table}[H]
\caption{Formal Definition of Reinforcement Learning\cite{David_Silver:intro_RL}}
\centering
\begin{tabular}{c c} % centered columns (4 columns)
\hline Symbols & Explanation \\[0ex] \hline
$a$ 	& agent \\
$e$ 	& environment \\
$t$ 	& timestamp, increments at each step \\
$O_t^a$ & agent's observation(input) from environment at step $t$ \\
$A_t^a$ & agent's actions(ouput) to environment at step $t$ \\
$R_t^a$ & agent's reward from environment at step $t$, is a scalar feedback signal \\
$S_t^a$ & agent's state at step $t$ \\
$S_t^e$ & environment's state at step $t$ \\
\end{tabular}
\label{table:srl}
\end{table}

\subsection{Inperfect Information Game}
In full observability game, agent directly observes the environment stat, where:
\begin{eqnarray}
O_{t}^a=S_{t}^a=S_{t}^e
\end{eqnarray}
As mentioned in 1.2.2, because of the game mechanisms: \emph{fog-of-war} and confusing magic(invisibility, hallucination, etc.), agents receive inperfect information from partially observed environment, which leads more challenging AI design than \emph{Chess} and \emph{Go}.
\begin{figure}[H]
\caption{1v1 Adversarial RL Framework}
\centering
\includegraphics
[width=0.618\textwidth]
{./img/Adversarial_Reinforcement_Learning.pdf}
\end{figure}
where:
\begin{eqnarray}
S_{1,t}^{a} \neq S_{2,t}^{a}\\
S_{1,t}^{a} \cup S_{2,t}^{a} = S_t^e
\end{eqnarray}

\section{Developing Environment}
\subsection{Tools}
\subsubsection{BWAPI}
The Brood War Application Programming Interface(BWAPI) is a free and open source C++ framework used to interact with Starcraft: BroodWar$^\circledR$. Researchers can create AI agents to play the game.\cite{GitHub:bwapi}
\subsubsection{Torch}
\emph{Torch} is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first.
Torch is mainly implemented by \emph{LuaJIT}.\cite{GitHub:torch}
\subsubsection{TorchCraft}
\emph{TorchCraft} is a bridge between Torch and StarCraft.\cite{GitHub:TorchCraft}\cite{DBLP:journals/corr/SynnaeveNACLLRU16}
\subsubsection{TorchCraft-Py}
A Python wrapper for TorchCraft developed by Alibaba.\cite{GitHub:torchcraft-py}
\subsubsection{gym}
An open source reinforcement framework
\subsubsection{gym-starcraft}

\subsection{Architecture}
\begin{figure}[H]
\caption{TorchCraft-based Client-Server Architecture}
\centering
\includegraphics
[width=0.618\textwidth]
{./img/Client_Server_Architecture.pdf}
\end{figure}

\subsection{BWAPI}
\subsubsection{Client}
\subsubsection{ServerState}
\subsubsection{Frame}
\paragraph{Order}
\paragraph{Unit}
\paragraph{Resources}
\paragraph{Bullet}
\paragraph{Action}

\section{Macromanagement AI}
\subsection{Build Order Planning}
\subsubsection{Search Problem}
the goal is to find the build order that optimizes a specific heuristic.
\paragraph{Notes and Comments.}
\subsection{Dataset}
\subsubsection{Format}
Replay files are in binary format and require preprocessing before knowledge can be extracted.
\subsection{Learning from replays}

\subsection{}

\begin{codebox}
\Procname{Algorithm 2: $\proc{Insertion-Sort}(A)$}
\li \For $j \gets 2$ \To $\attrib{A}{length}$
\li \Do
$\id{key} \gets A[j]$
\li \Comment Insert $A[j]$ into the sorted sequence
$A[1 \twodots j-1]$.
\li $i \gets j-1$
\li \While $i > 0$ and $A[i] > \id{key}$
\li \Do
$A[i+1] \gets A[i]$
\li $i \gets i-1$
\End
\li $A[i+1] \gets \id{key}$
\End
\end{codebox}
%------------------------------

\section{Micromanagement AI}
\subsection{}

% ---- Bibliography ----
\bibliographystyle{unsrt}% plain, unsrt, alpha, abbrv
\bibliography{starcraft}

\end{document}